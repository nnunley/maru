- the _pipe functions could be simplified

- why is snow-demo 10% faster when rendering 500/505 flakes or changing
    _work_until_below to trigger when q.n > 28 (as opposed to 27)?

- when should we just fix block size in nile.h so that we don't have to
  do the .data trick anymore? (then we can get rid of BAT)

- change nile_Real_t to typedef to float (for vectorizing)
    - and fix the places I cast to float

- optimizations
    - in SortBy, perform the shifting in a single pass rather than
      one at a time (in the insertion sort)
    - in SortBy, we can specialize on quantum (and index)
    - why is sortby so slow? where is the hot spot? Is the .data array
      approach complicating things?
    - only acquire locks when needed. Do a simple check before locking
      (only for common locks)
    - if Zip is taking too long, specialize the inner loop for certain
      quantum combinations, and if not enough, hand vectorize the inner loop.
      (or figure out how to get gcc to autovectorize it, since it
      complains about interleaving access patterns)
    - skip the q in some cases in nile_Process_schedule (just _run the process
      straightaway)
    - prefetch
    - data parallel cloning
    - (for multiprocessors) use thread affinity. visit other threads
      in local processor, then check shared q, then the rest of the threads.
    - make prefix_input split the buffer
    - make SortBy use (a) jump pointer(s) to move around faster during
      the first phase of the search (if that part of the function is hot)
    - we may have too much stealing.
      should we look in the shared q first instead of our peer qs (like
      traditional work stealing)?
      or perhaps only steal from peer if peer's q has more than one process
      in it? (but what about future data parallel plans?)
    - for very light workloads:
        - maybe the threads are just ping-ponging cache lines?
        - might we be losing time on just moving chunks between the two threads
          heaps?
        - everything takes longer because we're not working in the L1 cache very well?
        - if we spend a large part of our time issuing atomic instructions,
          then adding an additional thread might just slow things down,
          since it just makes the atomic instructions take longer? do we
          spend more time contending for locks? do we issue more atomic
          instructions per pipeline when there is more than one thread?
          might the hardware optimize atomic instructions/reads/writes if there is only
          one thread in the address space?
        - skinny benchmarks may also be affected by the spin locks,
            because _remove and _enqueue show up high on the profile,
            yet they don't do much
        - do we slow things down because of the constant polling of the
          shared q?
        - Why is Process_pipe() taking twice as long compared to the
          single thread version? This makes no sense
        - long pipelines with little workload can't get past speedup of 1.8 (for
          two threads), why?

- tests
    - time threads disabled vs. 1 thread
    - run with valgrind
    - try different stealing approaches
        - linear search
        - steal from shared q first
        - don't steal if only one in q, then try the shared q, then
          back to peers
    - different types of pipelines:
        - short/long pipelines
        - light/heavy workloads
        - few/many pipelines
        - few/many rounds
        - varied (from process to process) workloads
        - varied input/output rate
        - short/long input data
        - gated pipelines

- internal parameters to tweak
    - block size
    - chunk size
    - input quota
    - input quota max
    - input quota max for leading process
    - shared q quota (max/min)
    - exponential backoff in Thread_main

- what about unused process references (which means they won't be freed).
  process body might contain a swap that uses parameters, but
  we never reach the swap!, also the epilogue. Should we generate an epilogue
  that frees the references? We don't reach (invisible bottom of) the epilogue
  if the swap is successful....What about different branches in the body
  that use different process arguments (of type process)?

- would it be cleaner if we had a loop "allowance" to keep processes from
  running too long w/out checking in with the outer loop?

- naming
    - I still don't like the term "process"
    - swap/swapped is a poor choice because of what a swapped out process means
      in OS land
    - _enqueue_output and _append_output are have names that are too similar
    - what do we call process forwarding now?
        NILE_REMOVED,  NILE_SUBSTITUTED, supercede, supplant, yield, resign, retire,
            defer, delegate, forward, replace, finished
        process...
            - substitution (substitute)
            - swap (switch)
            - yielding
            - cession (cede)
            - supercede
            - consession (concede)
            - replacement
            - resignation (resign)
            - retire
            - replace(ment)
            - input forwarding
            - similar to tail call (elimination)
    - reached/hit/within/met/at limit/quota

- we can calculate approx. how many blocks we'll need (discounting
  hoarders like SortBy) using this formula:

      nprocesses + 2 * inputquota * nprocesses + 2 * chunksize * nthreads
