- change nile_Buffer to take a process instead of a node
- random jump address (probably from corrupt process block) does this only
  happen when we've run out of memory? (in which case maybe I'm not checking
  for NULL somewhere?)
- DupZip
    - allow for NULL process args (to avoid having to passing Identity)
- finish Tap
- figure out a replacement for how I did pen stroking before
- funnel shouldn't need to take quantum, it should just look downstream

- optimizations
      - why is sortby so slow? where is the hot spot? Is the .data array
        approach complicating things?
    - in SortBy, perform the shifting in a single pass rather than
      one at a time (in the insertion sort)
    - only lock when needed. Do a simple check before locking.
    - prefetch
    - data parallel cloning
    - (for multiprocessors) use thread affinity. visit other threads
      in local processor, then check shared q, then the rest of the threads.
    - The Zip part of DupZip could be faster if we had specialized
      versions for each quantum (eliminating the inner loop, so the compiler
      can unroll)
    - make prefix_input split the buffer
    - make SortBy use (a) jump pointer(s) to move around faster during
      the first phase of the search
    - we may have too much stealing.
      should we look in the shared q first instead of our peer qs (like
      traditional work stealing)?
      or perhaps only steal from peer if peer's q has more than one process
      in it? (but what about future data parallel plans?)
    - for very light workloads:
        - maybe the threads are just ping-ponging cache lines?
        - might we be losing time on just moving chunks between the two threads
          heaps?
        - everything takes longer because we're not working in the L1 cache very well?
        - if we spend a large part of our time issuing atomic instructions,
          then adding an additional thread might just slow things down,
          since it just makes the atomic instructions take longer? do we
          spend more time contending for locks? do we issue more atomic
          instructions per pipeline when there is more than one thread?
          might the hardware optimize atomic instructions/reads/writes if there is only
          one thread in the address space?
        - skinny benchmarks may also be affected by the spin locks,
            because _remove and _enqueue show up high on the profile,
            yet they don't do much
        - do we slow things down because of the constant polling of the
          shared q?
        - Why is Process_pipe() taking twice as long compared to the
          single thread version? This makes no sense
        - long pipelines with little workload can't get past speedup of 1.8 (for
          two threads), why?

- tests
    - time threads disabled vs. 1 thread
    - run with valgrind
    - try different stealing approaches
        - linear search
        - steal from shared q first
        - don't steal if only one in q, then try the shared q, then
          back to peers
    - different types of pipelines:
        - short/long pipelines
        - light/heavy workloads
        - few/many pipelines
        - few/many rounds
        - varied (from process to process) workloads
        - varied input/output rate
        - short/long input data
        - gated pipelines

- internal parameters to tweak
    - block size
    - chunk size
    - input quota
    - input quota max
    - input quota max for leading process
    - shared q quota (max/min)
    - exponential backoff in Thread_main

- what about unused process references (which means they won't be freed).
  process body might contain a swap that uses parameters, but
  we never reach the swap!, also the epilogue. Should we generate an epilogue
  that frees the references? We don't reach (invisible bottom of) the epilogue
  if the swap is successful....What about different branches in the body
  that use different process arguments (of type process)?

- would it be cleaner if we had a loop "allowance" to keep processes from
  running too long w/out checking in with the outer loop?

- naming
    - I still don't like the term "process"
    - swap/swapped is a poor choice because of what a swapped out process means
      in OS land
    - _enqueue_output and _append_output are have names that are too similar
    - what do we call process forwarding now?
        NILE_REMOVED,  NILE_SUBSTITUTED, supercede, supplant, yield, resign, retire,
            defer, delegate, forward, replace, finished
        process...
            - yielding
            - cession (cede)
            - supercede
            - consession (concede)
            - replacement
            - resignation (resign)
            - retire
            - substitution (substitute)
            - replace(ment)
            - swap (switch)
            - input forwarding
            - similar to tail call (elimination)
    - reached/hit/within/met/at limit/quota

- we can calculate approx. how many blocks we'll need (discounting
  hoarders like SortBy) using this formula:

      nprocesses + 2 * inputquota * nprocesses + 2 * chunksize * nthreads
